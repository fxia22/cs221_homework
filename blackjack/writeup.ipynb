{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "#### 1a\n",
    "\n",
    "At 0 iteration, all $ V_{opt}(s)$ are set to 0.\n",
    "\n",
    "Using the following formular\n",
    "$$ V_{opt}^{(t)}(s) \\leftarrow{\\max_{a \\in Actions(s)}} \\underbrace{\\sum_{s'} T(s, a, s') [Reward(s, a, s') + \\gamma V_{opt}^{(t-1)}(s')]}_{Q_{opt}^{(t-1)}(s, a)} $$\n",
    "\n",
    "The value of $V_{opt}$ can be expressed in the following table, excluding the terminal state\n",
    "\n",
    "iteration/$S$ | 0 | 1 | 2 | 3\n",
    "---|---|--- | --- | ---\n",
    "-1 | 0 | 15 | 14  | 17.69\n",
    "Q(-1, +1)| 0 | 15 | 12.5 | 16.535\n",
    "Q(-1, -1) | 0 | 12.5 | 14 | 17.69\n",
    "0  | 0 |  -5 | 13.45| 11.7\n",
    "Q(0, +1)| 0| -5 | 12.3 | 10.8\n",
    "Q(0, -1)| 0| -5 | 13.45 | 11.7\n",
    "1  | 0 | 26.5  | 23 | 35.915\n",
    "Q(1, +1)| 0| 26.5 | 23 | 35.015\n",
    "Q(1, -1)| 0| 16 | 12 | 26.76\n",
    "\n",
    "####1b \n",
    "The optimal policy after iteration 2 is\n",
    "\n",
    "state| Policy\n",
    "--- | ---\n",
    "-1  | -1\n",
    "0   | +1\n",
    "1   | +1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2\n",
    "#### 2.\n",
    "\n",
    "If the MDP problem is acyclic, we can present the states in a sort of tree graph, where the leaf nodes are the final states in the MDP, and the parent nodes are the intermediate states, and the root is the start state. Since the leaf nodes have no dependency with the past policy value, we can start Value Iteration from the leaf nodes and then propagate upwards, layer by layer, to get all the value in a single iteration.\n",
    "\n",
    "#### 3.\n",
    "\n",
    "Take the original equation to calculate $Q_{opt}$\n",
    "\n",
    "$Q_\\text{opt}(s, a) = \\sum_{s'} T(s, a, s') [\\text{Reward}(s, a, s') + \\gamma V_\\text{opt}(s')]$ \n",
    "\n",
    "We can expand the summation into \n",
    "\n",
    "$Q_\\text{opt}(s, a) = \\sum_{s'} T(s, a, s') \\text{Reward}(s, a, s') +  \\sum_{s'} T(s, a, s')\\gamma V_\\text{opt}(s')$ \n",
    "\n",
    "The goal is to find a solver that can solve an equation with $T'$ and $\\text{Reward}'$ such that\n",
    "Keep the second term in the equation unchanged\n",
    "\n",
    "$$ \\sum_{s'} T'(s, a, s') V_\\text{opt}(s') = \\sum_{s'} T(s, a, s')\\gamma V_\\text{opt}(s') $$\n",
    "\n",
    "To keep the first term in the equation unchanged\n",
    "\n",
    "$$ \\sum_{s' \\in \\text{States}}T'(s, a, s') \\text{Reward}'(s, a, s') + T'(s, a, s_{new}) \\text{Reward}'(s, a, s_{new})  = T(s, a, s') \\text{Reward}(s, a, s') $$\n",
    "\n",
    "The other condition we have to satisfy when creating a new state\n",
    "$$ \\sum_{s' \\in \\text{States}} T'(s, a, s') = 1$$\n",
    "$$ V_{opt}(s_{new}) = 0 $$\n",
    "\n",
    "\n",
    "We can see the solution is\n",
    "$$ T'(s, a, s') = \\gamma T(s, a, s') $$\n",
    "$$ \\text{Reward}'(s, a, s') = \\frac{\\text{Reward}(s, a, s')}{\\gamma} $$\n",
    "$$ T'(s, a, s_{new}) = 1 - \\gamma $$\n",
    "$$\\text{Reward}'(s, a, s_{new}) = 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "#### b.\n",
    "For smallMDP, the policy produced by QLearning is have 96% similar with the value iteration method.\n",
    "For largeMDP, the policy produced is only 77% similar.\n",
    "\n",
    "Increase the number of trials to 300000 does not improve the accuracy of QLearning either.  \n",
    "\n",
    "Since in our feature extractor, we are simply mapping states to actions, this provides no generalization, and it presents the QLearning algorithm with too large of a state space to explore. \n",
    "\n",
    "\n",
    "#### d.\n",
    "Running the optimal policy obtained from originalMDP produced an average utility of 7.24 on the newThresholdMDP, which is similar to 7.22 from the originalMDP.\n",
    "\n",
    "Running QLearning with identityFeatureExtractor increased the average utility to 8.79, which increased the reward closer to the maximal possible value obtained from value iteration on the newThresholdMDP, which is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
