{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Problem 1\n",
    "\n",
    "####Section a\n",
    "Taking the gradient of the loss function, we have \n",
    "$$ \\nabla_\\mathbb{w} \\text{Loss}_{\\text{hinge}} =\n",
    "  \\begin{cases}\n",
    "    - \\phi(x) y      & \\quad \\text{if } 1 - \\mathbb{w} \\cdot \\phi(x) y\\ > 0\\\\\n",
    "    0  & \\quad \\text{otherwise } \\\\\n",
    "  \\end{cases}\n",
    "$$\n",
    "  \n",
    "List results after each sample\n",
    "    1. (+1) pretty good\n",
    "\n",
    "$1 - \\mathbb{w} \\cdot \\phi(x) y = 0$, so gradient for both $w$'s corresponding element to 'pretty' and 'good' is -1\n",
    "\n",
    "$w$ = {pretty: 1, good:1}\n",
    "\n",
    "    2. (-1) bad plot\n",
    "Again the loss function is 0. But this time the gradients are 1 because the sign of $y$ flipped\n",
    "\n",
    "$w$ = {pretty: 1, good:1, bad: -1, plot: -1}\n",
    "\n",
    "    3. (+1) not bad\n",
    "\n",
    "$1 - \\mathbb{w} \\cdot \\phi(x) y = 1 - (-1) \\times 1 = 2$, gradients are -1, same procedure as (1)\n",
    "\n",
    "$w$ = {pretty: 1, good:1, bad: 0, plot: -1, not: 1}\n",
    "\n",
    "    4. (+1) pretty scenery\n",
    "\n",
    "$1 - \\mathbb{w} \\cdot \\phi(x) y = 1 - (-1) \\times 1 = 2$, same as above, -1\n",
    "\n",
    "$w$ = {pretty: 2, good:1, bad: 0, plot: -1, not: 1, pretty: 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section b\n",
    "Proposed Data set\n",
    "\n",
    "- (+1) good\n",
    "- (-1) bad\n",
    "- (-1) not good\n",
    "- (+1) not bad\n",
    "\n",
    "Assume $f(\\text{word})$ represents the dot product of $ \\text{weight} \\times \\text{word_feature}$. To ensure the examples get 0 errors after training, we need to satisfy\n",
    "\n",
    "$$ f(\\text{good}) > 0 $$\n",
    "$$ f(\\text{bad}) < 0 $$\n",
    "$$ f(\\text{not}) + f(\\text{good}) < 0 $$\n",
    "$$ f(\\text{not}) + f(\\text{bad}) > 0 $$\n",
    "\n",
    "If we take equation 1, 2 to be true, then to satisfy 3, $f(\\text{not})$ needs to be positive, but equation 4 requires it to be negative. Thus no simple word feature can reduce the error to 0\n",
    "\n",
    "To solve this issue, we can add a double word feature. Then equation 3 and 4 become\n",
    "\n",
    "$$ f(\\text{not}) + f(\\text{good}) + f(\\text{not good})< 0 $$\n",
    "$$ f(\\text{not}) + f(\\text{bad}) + f(\\text{not bad})> 0 $$\n",
    "\n",
    "Then the equations above can be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 2\n",
    "\n",
    "#### Section a\n",
    "$ \\text{Loss}(x,y,w) = (y - \\sigma(\\mathbb w \\cdot \\phi(x))^2 = (y - (1 + e^{-(\\mathbb w \\cdot \\phi(x))})^{-1})^2$\n",
    "\n",
    "#### Section b\n",
    "Using chain rule\n",
    "\n",
    "$ \\frac{\\partial (y - \\sigma(\\mathbb w \\cdot \\phi(x)))^2}{\\partial \\mathbb w} = -2 * (y - \\sigma(\\mathbb w \\cdot \\phi(x)) * \\sigma(\\mathbb w \\cdot \\phi(x)) * (1- \\sigma(\\mathbb w \\cdot \\phi(x))) * \\|\\phi(x)\\| $\n",
    "\n",
    "#### section c\n",
    "\n",
    "If $ y = 0$, the equation above can be simplified as\n",
    "$ 2 * \\sigma(\\mathbb w \\cdot \\phi(x)) ^2 * (1- \\sigma(\\mathbb w \\cdot \\phi(x))) * \\|\\phi(x)\\| $\n",
    "\n",
    "Because the range of the $\\sigma(z)$ function is $(0,1)$. When $\\|\\mathbb w \\cdot \\phi(x)\\|$ is too large, $\\sigma(\\mathbb w \\cdot \\phi(x))$ approaches 0 or 1, the equation above will have a close to 0 value.\n",
    "\n",
    "#### section d\n",
    "Looking at the coefficient of $\\|\\phi(x)\\|$\n",
    "$ \\frac{\\partial 2  \\sigma^2(z) (1- \\sigma(z))}{\\partial z}\n",
    "= 2(2 \\sigma(z) - 3 \\sigma^2(z)) $\n",
    "\n",
    "When the original equation reachs maximum, the derivative should reach 0\n",
    "\n",
    "Thus we can solve\n",
    "$\\sigma(z) = \\sqrt{\\frac{2}{3}}$\n",
    "\n",
    "So the maximum value of the Loss function, when y=0, is\n",
    "$ \\sqrt{\\frac{2}{3}}\\|\\phi(x)\\| $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Problem 3\n",
    "\n",
    "#### section d\n",
    "```python\n",
    "    numIters = 15  \n",
    "    stepSize = 0.04\n",
    "```\n",
    "```\n",
    "At iteration 0, loss on training is 0.233540, loss on predictor is 0.341587 \n",
    "At iteration 1, loss on training is 0.156443, loss on predictor is 0.306415 \n",
    "At iteration 2, loss on training is 0.152504, loss on predictor is 0.310636 \n",
    "At iteration 3, loss on training is 0.085256, loss on predictor is 0.287563 \n",
    "At iteration 4, loss on training is 0.060777, loss on predictor is 0.283905 \n",
    "At iteration 5, loss on training is 0.070906, loss on predictor is 0.286156 \n",
    "At iteration 6, loss on training is 0.065279, loss on predictor is 0.307541 \n",
    "At iteration 7, loss on training is 0.045020, loss on predictor is 0.291784 \n",
    "At iteration 8, loss on training is 0.037985, loss on predictor is 0.281373 \n",
    "At iteration 9, loss on training is 0.019696, loss on predictor is 0.277997 \n",
    "At iteration 10, loss on training is 0.015476, loss on predictor is 0.269837 \n",
    "At iteration 11, loss on training is 0.014069, loss on predictor is 0.268711 \n",
    "At iteration 12, loss on training is 0.020540, loss on predictor is 0.279122 \n",
    "At iteration 13, loss on training is 0.008160, loss on predictor is 0.270118 \n",
    "At iteration 14, loss on training is 0.007597, loss on predictor is 0.274620 \n",
    "```\n",
    "\n",
    "#### section e\n",
    "> === wickedly funny , visually engrossing , never boring , this movie challenges us to think about the ways we consume pop culture .\n",
    "\n",
    "model does not understand 'never boring', only took 'boring' as negative\n",
    "\n",
    "> === rain is a small treasure , enveloping the viewer in a literal and spiritual torpor that is anything but cathartic .\n",
    "\n",
    "'topor' and 'anything' have too much negativity in other reviews\n",
    "\n",
    ">=== patchy combination of soap opera , low-tech magic realism and , at times , ploddingly sociological commentary .\n",
    "\n",
    "'realism', 'magic' are positive words, but 'low-tech magic realism' is not\n",
    "\n",
    ">=== only in its final surprising shots does rabbit-proof fence find the authority it's looking for .\n",
    "\n",
    "'does' had a weight of 1, but it is really neutral, only a word for emphasize\n",
    "\n",
    "> === . . . standard guns versus martial arts cliche with little new added .\n",
    "\n",
    "new's interpretation didn't take 'little' into account\n",
    "\n",
    ">=== even during the climactic hourlong cricket match , boredom never takes hold .\n",
    "\n",
    "'boredom never takes hold' is compliment, but each word was categorized as negative\n",
    "\n",
    ">=== even a hardened voyeur would require the patience of job to get through this interminable , shapeless documentary about the swinging subculture .\n",
    "\n",
    "'shapeless' is negative in context, but the model think otherwise\n",
    "\n",
    ">=== alternative medicine obviously has its merits . . . but ayurveda does the field no favors .\n",
    "\n",
    "'does' is too positive\n",
    "\n",
    ">=== no screen fantasy-adventure in recent memory has the showmanship of clones' last 45 minutes .\n",
    "\n",
    "'no' negated the meaning of sentence, but model only took the literal meaning\n",
    "\n",
    ">=== a clever blend of fact and fiction .\n",
    "\n",
    "'fiction' here is a genre, but the model took the negative notation seriously\n",
    "\n",
    "\n",
    "#### section g\n",
    "\n",
    "When $n=5$, the error is the smallest, any number bigger or smaller increases the error.\n",
    "\n",
    "Doing a histogram of word character length based on space or tab split, we can see most words have 5 or more characters\n",
    "\n",
    "{1: 48, 2: 113, 3: 362, 4: 998, 5: 1437, 6: 1643, 7: 1780, 8: 1502, 9: 1184, 10: 931, 11: 658, 12: 460, 13: 275, 14: 146, 15: 100, 16: 49, 17: 28, 18: 21, 19: 10, 20: 6, 21: 2, 22: 1, 23: 3, 24: 3, 25: 1, 26: 2}\n",
    "\n",
    "So using 5 can pick out most of meaningful words or their word stems. Picking 4 or less reduced the number of word in the weights. Picking 6 increased the weights count to 171735, vs 112742 at 5, too many weights for little data\n",
    "\n",
    "I believe sample like 'not good' will have a better performance with n-gram, since n-gram can capture 'not' as a prefix to subsequent words\n",
    "\n",
    "#### section f\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### problem 4\n",
    "#### section a\n",
    "\n",
    "1. $\\mu_1 = [0, -1]$ and $\\mu_2 = [2, 2]$\n",
    "```\n",
    "distance^2 of [1,0] to each centers [2, 5]\n",
    "assigning to center  0\n",
    "distance^2 of [2,1] to each centers [8, 1]\n",
    "assigning to center  1\n",
    "distance^2 of [0,0] to each centers [1, 8]\n",
    "assigning to center  0\n",
    "distance^2 of [0,2] to each centers [9, 4]\n",
    "assigning to center  1\n",
    "moving center from [0,-1] to [0.5,0.0]\n",
    "moving center from [2,2] to [1.0,1.5]\n",
    "iteration 0 with loss 8\n",
    "assignment [0, 1, 0, 1]\n",
    "distance^2 of [1,0] to each centers [0.25, 2.25]\n",
    "assigning to center  0\n",
    "distance^2 of [2,1] to each centers [3.25, 1.25]\n",
    "assigning to center  1\n",
    "distance^2 of [0,0] to each centers [0.25, 3.25]\n",
    "assigning to center  0\n",
    "distance^2 of [0,2] to each centers [4.25, 1.25]\n",
    "assigning to center  1\n",
    "moving center from [0.5,0.0] to [0.5,0.0]\n",
    "moving center from [1.0,1.5] to [1.0,1.5]\n",
    "iteration 1 with loss 3.0\n",
    "assignment [0, 1, 0, 1]\n",
    "distance^2 of [1,0] to each centers [0.25, 2.25]\n",
    "assigning to center  0\n",
    "distance^2 of [2,1] to each centers [3.25, 1.25]\n",
    "assigning to center  1\n",
    "distance^2 of [0,0] to each centers [0.25, 3.25]\n",
    "assigning to center  0\n",
    "distance^2 of [0,2] to each centers [4.25, 1.25]\n",
    "assigning to center  1\n",
    "moving center from [0.5,0.0] to [0.5,0.0]\n",
    "moving center from [1.0,1.5] to [1.0,1.5]\n",
    "```\n",
    "2. $\\mu_1 = [2, 0]$ and $\\mu_2 = [-1, 0]$\n",
    "```\n",
    "distance^2 of [1,0] to each centers [4, 1]\n",
    "assigning to center  1\n",
    "distance^2 of [2,1] to each centers [10, 1]\n",
    "assigning to center  1\n",
    "distance^2 of [0,0] to each centers [1, 4]\n",
    "assigning to center  0\n",
    "distance^2 of [0,2] to each centers [5, 8]\n",
    "assigning to center  0\n",
    "moving center from [-1,0] to [0.0,1.0]\n",
    "moving center from [2,0] to [1.5,0.5]\n",
    "iteration 0 with loss 8\n",
    "assignment [1, 1, 0, 0]\n",
    "distance^2 of [1,0] to each centers [2.0, 0.5]\n",
    "assigning to center  1\n",
    "distance^2 of [2,1] to each centers [4.0, 0.5]\n",
    "assigning to center  1\n",
    "distance^2 of [0,0] to each centers [1.0, 2.5]\n",
    "assigning to center  0\n",
    "distance^2 of [0,2] to each centers [1.0, 4.5]\n",
    "assigning to center  0\n",
    "moving center from [0.0,1.0] to [0.0,1.0]\n",
    "moving center from [1.5,0.5] to [1.5,0.5]\n",
    "iteration 1 with loss 3.0\n",
    "```\n",
    "\n",
    "code \n",
    "```python\n",
    "def distance2(a,b):\n",
    "    return sum([(a[x]-b[x])**2 for x in range(2)])\n",
    "def kmeans(examples, centers):\n",
    "    maxIters = 100\n",
    "    for counter in xrange(maxIters):\n",
    "        square_loss = 0\n",
    "        new_assignment = []\n",
    "        for i, k in enumerate(examples):\n",
    "            dist_all = [distance2(k, c) for c in centers]\n",
    "            print \"distance^2 of [\" + str(k[0]) + \",\" + str(k[1]) + \"] to each centers\", dist_all\n",
    "            min_idx, min_dist2 = min(enumerate(dist_all), key = lambda p: p[1])\n",
    "            print \"assigning to center  \" + str(min_idx)\n",
    "            new_assignment.append(min_idx)\n",
    "            square_loss = square_loss + min_dist2\n",
    "        # update step\n",
    "        new_centers = []\n",
    "        # TODO possible to have empty point sets?\n",
    "        for c, center in enumerate(centers):\n",
    "            centered_points = [examples[ex] for ex, cen in enumerate(new_assignment) if cen == c]\n",
    "\n",
    "            sumDistance = [sum([a[m] for a in centered_points]) for m in range(2)]\n",
    "            #sumDistance = sumByKey(centered_points)\n",
    "            new_centers.append([(v/float(len(centered_points))) for v in sumDistance])\n",
    "            print \"moving center from \" + ptostring(center) + \" to \" + ptostring(new_centers[-1])\n",
    "            #print centered_points\n",
    "        centers = new_centers\n",
    "        assignment = new_assignment\n",
    "        print \"iteration \" + str(counter) + \" with loss \" + str(square_loss)\n",
    "        #print \"center\", centers\n",
    "        print \"assignment\", assignment\n",
    "        if ( counter > 0 and square_loss / last_square_loss > 0.99 ):\n",
    "            break\n",
    "        else:\n",
    "            last_square_loss = square_loss\n",
    "    return\n",
    "\n",
    "\n",
    "def ptostring(k):\n",
    "    return \"[\" + str(k[0]) + \",\" + str(k[1]) + \"]\"\n",
    "\n",
    "points = [ [1,0],[2,1],[0,0],[0,2]]\n",
    "kmeans(points, [[0,-1],[2,2]])\n",
    "kmeans(points, [[-1,0],[2,0]])\n",
    "```\n",
    "\n",
    "#### section c\n",
    "\n",
    "Preprocess S into clustered points $S$ where $S_i$ contains the points of a same cluster. \n",
    "Calculate $ c_i = \\text{average of cluster} S_i$ \n",
    "\n",
    "Then in the update step, we update the center to\n",
    "$ \\mu_j = \\text{average of } c_j $, where $c_j$ is assigned to center $\\mu_j$ from the assignment step.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
