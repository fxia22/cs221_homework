{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Weiyi Zheng\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "<li class=\"writeup\">\n",
    "Let $b_1, \\dots, b_n$ be real numbers representing positions on a number line.\n",
    "Let $w_1, \\dots, w_n$ be positive numbers representing the importance of these positions.\n",
    "Define the quadratic function: $f(x) = \\frac{1}{2} \\sum_{i=1}^n w_i (x - b_i)^2$.\n",
    "What value $x$ minimizes $f(x)$?\n",
    "You can think about this problem as trying to find the point $x$ that's not too far\n",
    "away from the points $b_i$'s.\n",
    "Over time, hopefully you'll appreciate how nice quadratic functions are to minimize.\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The derivative of the quadratic function is $f'(x)=\\sum\\limits_{i=1}^n w_i(x-b_i)$, $f(x)$ is minimized when the linear distance when the distance between $x$ and highly weighted $b_i$ are minimized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<li class=\"writeup\">\n",
    "In this class, there will be a lot of sums and maxes.\n",
    "Let's see what happens if we switch the order.\n",
    "Let $f(x) = \\max_{a \\in \\{1,-1\\}} \\sum_{j=1}^d a x_j$\n",
    "and $g(x) = \\sum_{j=1}^d \\max_{a \\in \\{1,-1\\}} a x_j$,\n",
    "where $x = (x_1, \\dots, x_d) \\in \\mathbb{R}^d$ is a real vector.\n",
    "Does $f(x) \\le g(x)$, $f(x) = g(x)$, or $f(x) \\ge g(x)$ hold for all $x$?\n",
    "Prove it.\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:For All $x$ $f(x) \\le g(x)$ holds.\n",
    "\n",
    "**Prove**: $\\forall b \\in \\mathbb{R} $, $\\max_{a \\in \\{1,-1\\}} a  b$ equals to $b$ when $b \\ge 0$, $-b$ when $b < 0$. Thus we can treat $f(x)$ as $f(x) = | \\sum_{j=1}^d x_j |$, $g(x) = \\sum_{j=1}^d | x_j |$. \n",
    "\n",
    "If all $x_j \\ge 0$, Then $f(x) = g(x)$. If not, $f(x) < g(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<li class=\"writeup\">\n",
    "Suppose you repeatedly roll a fair six-sided dice until the number of dots is $3$ or fewer (and then you're done).\n",
    "Every time the dice turns up with a $6$, you earn $r$ points.\n",
    "What is the expected number of total points (as a function of $r$) that you will earn before you're done?\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: If dice roll is n, The probability of rolling a 6 should be $\\mathbb{P}(\\text{rolling a 6 per game that can continue}) = \\mathbb{P}(n=6|n=\\{4,5,6\\}) = 1/3$. The total expected probability of being able to end the game at nth roll is $\\frac{1}{2}^{n-1} \\frac{1}{2} = \\frac{1}{2}^n$. So the expected value\n",
    "\n",
    "$\\mathbb{E}(\\text{points}) \n",
    "= r \\times P(\\text{4-6 for n-1 rolls})  \\times P(\\text{6|{4,5,6}}) \\times P(\\text{1-3 at nth roll}) $\n",
    "\n",
    "$= r \\times (n-1) \\times \\frac{1}{3} \\times \\frac{1}{2}^n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<li class=\"writeup\">\n",
    "Suppose the probability of a coin turning up heads is $0 < p < 1$,\n",
    "and that we flip it 5 times and get $\\{ \\text{H}, \\text{T}, \\text{T}, \\text{H}, \\text{H} \\}$.\n",
    "We know the probability (likelihood) of obtaining this\n",
    "sequence is $L(p) = p (1-p) (1-p) p p = p^3 (1-p)^2$.\n",
    "Now let's go backwards and ask the question: what is the value of $p$ that maximizes $L(p)$?\n",
    "Hint: consider taking the derivative of $\\log L(p)$.\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the probability of $L(p)$ is the same as maximizing  $\\log L(p)$, we take its derivative\n",
    "\n",
    "$ \\frac{d\\log L(p) }{dp} = 3/p + 2/(p-1)$\n",
    "\n",
    "When the derivative reachs 0 we will have a maximum. So solve\n",
    "\n",
    "$ 0 = 3/p + 2/(p-1) , 0 < p < 1$\n",
    "\n",
    "$ p = 0.6 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<li class=\"writeup\">\n",
    "Let's practice taking gradients,\n",
    "which is a key operation for being able to optimize continuous functions.\n",
    "For $\\mathbf w \\in \\mathbb R^d$ and constants $a_i, b_j \\in \\mathbb R^d$ and $\\lambda \\in \\mathbb R$, define\n",
    "the scalar-valued function\n",
    "$$f(\\mathbf w) = \\sum_{i=1}^n \\sum_{j=1}^n (a_i^\\top \\mathbf w - b_j^\\top \\mathbf w)^2 + \\lambda \\|\\mathbf w\\|_2^2,$$\n",
    "where the vector is $\\mathbf w = (w_1, \\dots, w_d)$ and $\\|\\mathbf w\\|_2 = \\sqrt{\\sum_{j=1}^d w_j^2}$ is known as the L2 norm.\n",
    "Compute the gradient $\\nabla f(\\mathbf w)$, which is a vector-valued function (i.e., for each $\\mathbf w \\in \\mathbb R^d$, $\\nabla f(\\mathbf w) \\in \\mathbb R^d$).\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "$$\\frac{d\\sum\\limits_{j=1}^d w_j^2}{dw_c} = \n",
    "\\begin{cases}\n",
    "2 w_j & j = c\\\\\n",
    "0 & j \\ne c\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\frac{d\\|\\mathbf w\\|_2^2}{d \\mathbf w} = 2 \\mathbf w$$ \n",
    "\n",
    "We can use rule derivatives of sum equals the sum of derivatives to solve the first part. So the final equation is\n",
    "\n",
    "$$\\nabla f(\\mathbf w) = \\sum_{i=1}^n \\sum_{j=1}^n 2(a_i^\\top \\mathbf w - b_j^\\top \\mathbf w)(a_i - b_i) + 2 \\lambda  \\mathbf w\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2\n",
    "<li class=\"writeup\">\n",
    "Suppose we have an image of a human face consisting of $n \\times n$ pixels.\n",
    "In our simplified setting, a face consists of two eyes and one mouth,\n",
    "each represented as an arbitrary axis-aligned rectangle.  As we'd like\n",
    "to handle Picasso portraits too, there are no constraints on the location or\n",
    "size of the rectangles.\n",
    "How many possible faces (choice of its component rectangles) are there?\n",
    "In general, we only care about asymptotic complexity,\n",
    "so give your answer in the form of $O(n^c)$ or $O(c^n)$ for some integer $c$.\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since the eyes and mouth can overlap and no dimensionality constrains, we can assume finding one of the three rectangles that belongs to a face is independent of the other rectangles of the same face. Let's look at how many rectangles we can find in a $n \\times n$ matrix.\n",
    "\n",
    "Finding a rectangle of dimension $ a \\times b$, where $ 0 < {a,b} \\le n $, we should be able to find $ (n-a+1)(n-b+1) $ rectangles with either side has a width > 0.\n",
    "So for all possible values of $a$ and $b$, Total number of rectangles is\n",
    "$$\\sum\\limits_{a=1}^n\\sum\\limits_{b=1}^n (n-a+1)(n-b+1)$$\n",
    "\n",
    "Asymptoticlly, the equation above is equivalent to $O(n^4)$ (Each summation provides a $n(n-1)/2$ possible solution, which is in the order of $O(n^2)$)\n",
    "\n",
    "Since we have to pick 3 of such rectangles of each face, the total amount of possibility should be $ O(n^{4 \\times 3}) = O(n^{12})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<li class=\"writeup\">\n",
    "Suppose we have $n$ cities on the number line: $1, 2, \\dots, n$.\n",
    "Define a function $c(i, j)$ which returns the cost of going from city $i$\n",
    "to city $j$, and assume it takes constant time to compute.\n",
    "We want to travel from $1$ to $n$ via a set of intermediate cities, but only\n",
    "moving forwards.\n",
    "We can compute the minimum cost of doing this by defining\n",
    "the following recurrence: $f(j) = \\min_{1 < i < j} [c(i, j) + f(i)]$ for $j = 1, \\dots, n$.\n",
    "Give an algorithm for computing $f(n)$ for a fixed $n$ in the most efficient way.\n",
    "What is the runtime (just give the big-O)?\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Anwser**:\n",
    "The answer is similar to use Dijkstra's algorithm to find the shortest path. \n",
    "* let's define $g(j)$ to be the current cost of going from city 1 to j, and initialize all values to infinity.\n",
    "* Add city 1 to a queue.\n",
    "* Pop the first element off the queue, calculate the cost to reach all of its neighbors, which takes a max of O(n) to compute. Update $g(j) = min(g(i) + c(i,j), g(j))$. Add all the unvisited neighbors into the queue. \n",
    "* Repeat the step above until we have no more elements in the queue. The $g(j)$ value is the minimal cost to reach from 1 to $n$\n",
    "Since there are a total of n vertex to visit, our algorithm will run in $O(n^2)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Suppose we have an $n \\times n$ grid.\n",
    "How many ways are there to get from the upper-left corner to the lower-right corner\n",
    "if at each step you are only allowed to move down or right?\n",
    "Give your answer as a function of $n$.\n",
    "For example, if $n = 3$, then the answer is $6$.\n",
    "\n",
    "**Anwser**: In order to reach the bottom right corder, there have to be $n$ right and $n$ down in the total of 2n steps.\n",
    "\n",
    "This is equivalent to picking the combination \n",
    "$$ \\binom{n}{2n} = \\frac{(2n)!}{n!n!} = \\frac{(2n)!}{2n!} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Consider the scalar-valued function $f(\\mathbf w)$ from Problem 1e.\n",
    "Devise a strategy that first does preprocessing in $O(n d^2)$ time,\n",
    "and then for any given vector $\\mathbf w$,\n",
    "takes $O(d^2)$ time instead to compute $f(\\mathbf w)$.\n",
    "Hint: refactor the algebraic expression; this is a classic trick used in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "Inside the summation, we have \n",
    "$(a_i^T w - b_j^T w)^2 = w^T (a_i^T - b_j^T)^T (a_i^T - b_j^T) w \n",
    "= w^T (a_i a_i^T - a_i b_j^T - b_j a_i^T + b_j b_j^T) w$\n",
    "\n",
    "1. Since the summation of $\\sum_{i=1}^n \\sum_{j=1}^n$ does not affect $w$, we can pull $w$ out of the summation by associative rules\n",
    "2. Summing over i does not affect j, vice versa, so we can simplify $\\sum_{i=1}^n \\sum_{j=1}^n (a_i b_j^T) = \\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T$\n",
    "3. $\\sum_{j=1}^n b_j^T = (\\sum_{j=1}^n b_j)^T$, this also applies to $\\sum_{i=1}^n a_i$\n",
    "\n",
    "According to the three reasonings above, we can simplify \n",
    "\n",
    "$$ \\sum_{i=1}^n \\sum_{j=1}^n (a_i^\\top \\mathbf w - b_j^\\top \\mathbf w)^2 \n",
    "= \\sum_{i=1}^n \\sum_{j=1}^n w^T (a_i a_i^T - a_i b_j^T - b_j a_i^T + b_j b_j^T) w $$ \n",
    "$$= w^T (\\sum_{i=1}^n \\sum_{j=1}^n a_i a_i^T - a_i b_j^T - b_j a_i^T + b_j b_j^T) w$$\n",
    "$$= w^T (\\sum_{i=1}^n a_i a_i^T - \\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T - (\\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T)^T + \\sum_{j=1}^n b_j b_j^T) w $$\n",
    "\n",
    "The preprocessing step is then to compute the $d \\times d$ matrix generated by the terms inside the parenthese. Runtime for each term is\n",
    "\n",
    "- $O(nd^2)$ for $\\sum_{i=1}^n a_i a_i^T$ and $\\sum_{j=1}^n b_j b_j^T$\n",
    "- $O(2nd+d^2)$ for $\\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T$\n",
    "\n",
    "To compute $ \\mathbf M = (\\sum_{i=1}^n a_i a_i^T - \\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T - (\\sum_{i=1}^n a_i \\sum_{j=1}^n b_j^T)^T + \\sum_{j=1}^n b_j b_j^T), \\mathbf M \\in \\mathbf R^{d\\times d}$ is $O(nd^2)$\n",
    "\n",
    "Then we can change the formula of $f(w)$\n",
    "$$ f(\\mathbf w) = \\mathbf w^T \\mathbf M \\mathbf w + \\lambda \\|\\mathbf w\\|_2^2$$\n",
    "\n",
    "Multiplying vector of size $d$ with matrix of size $d \\times d$, the runtime is $O(d^2)$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
